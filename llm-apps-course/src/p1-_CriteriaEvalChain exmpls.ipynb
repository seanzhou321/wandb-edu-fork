{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdeb36-f04a-44b7-ab7a-701afd418b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: https://github.com/redhat-et/foundation-models-for-documentation/blob/master/notebooks/llm-evaluation/langchain-evaluation.ipynb\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType, Criteria\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61ab0ef3-f5c3-4a05-905f-7744b31b3110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<EvaluatorType.QA: 'qa'>,\n",
       " <EvaluatorType.COT_QA: 'cot_qa'>,\n",
       " <EvaluatorType.CONTEXT_QA: 'context_qa'>,\n",
       " <EvaluatorType.PAIRWISE_STRING: 'pairwise_string'>,\n",
       " <EvaluatorType.SCORE_STRING: 'score_string'>,\n",
       " <EvaluatorType.LABELED_PAIRWISE_STRING: 'labeled_pairwise_string'>,\n",
       " <EvaluatorType.LABELED_SCORE_STRING: 'labeled_score_string'>,\n",
       " <EvaluatorType.AGENT_TRAJECTORY: 'trajectory'>,\n",
       " <EvaluatorType.CRITERIA: 'criteria'>,\n",
       " <EvaluatorType.LABELED_CRITERIA: 'labeled_criteria'>,\n",
       " <EvaluatorType.STRING_DISTANCE: 'string_distance'>,\n",
       " <EvaluatorType.EXACT_MATCH: 'exact_match'>,\n",
       " <EvaluatorType.REGEX_MATCH: 'regex_match'>,\n",
       " <EvaluatorType.PAIRWISE_STRING_DISTANCE: 'pairwise_string_distance'>,\n",
       " <EvaluatorType.EMBEDDING_DISTANCE: 'embedding_distance'>,\n",
       " <EvaluatorType.PAIRWISE_EMBEDDING_DISTANCE: 'pairwise_embedding_distance'>,\n",
       " <EvaluatorType.JSON_VALIDITY: 'json_validity'>,\n",
       " <EvaluatorType.JSON_EQUALITY: 'json_equality'>,\n",
       " <EvaluatorType.JSON_EDIT_DISTANCE: 'json_edit_distance'>,\n",
       " <EvaluatorType.JSON_SCHEMA_VALIDATION: 'json_schema_validation'>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(EvaluatorType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57f66021-55dc-40ab-a2c1-6fcdc3f07e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10af99a1-123e-4b92-8729-d25b65a24337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning\": \"1. Conciseness:\\n- The submission is \\\"8\\\" which is not concise or to the point as it is not the correct answer to the input \\\"5+5?\\\"\\n- Therefore, the submission does not meet the criteria of conciseness.\\n\\nN\",\n",
      "  \"value\": \"N\",\n",
      "  \"score\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, llm=llm, criteria=\"conciseness\")\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(json.dumps(eval_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "502af68f-97c5-4458-8893-5ae3e92d2f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning\": \"1. Is the submission helpful, insightful, and appropriate?\\n- The submission provided an incorrect answer to the given task of 5+5, which is 10. \\n- The answer of 8 is not helpful, insightful, or appropriate in this context.\\n- Therefore, the submission does not meet the criteria.\\n\\nN\",\n",
      "  \"value\": \"N\",\n",
      "  \"score\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluator=load_evaluator(EvaluatorType.CRITERIA, llm=llm, criteria=\"helpfulness\")\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(json.dumps(eval_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23aba8af-73cd-462d-be3e-e6fe26c9c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning\": \"1. The input is a mathematical question \\\"5+5?\\\", which is numeric in nature.\\n2. The submission \\\"8\\\" is also numeric, as it is a numerical value.\\n\\nTherefore, the submission meets the criteria.\",\n",
      "  \"value\": \"Y\",\n",
      "  \"score\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "custom_crt={\"numeric\": \"Does the output contain numeric or mathematical information?\"}\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, llm=llm, criteria=custom_crt)\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(json.dumps(eval_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1096833-8ca9-484b-afd3-e6cdcd35258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning\": \"- numeric: The submission \\\"8\\\" contains numeric information, as it is a number.\\n- mathematical: The submission \\\"8\\\" does not contain the correct mathematical information, as the correct answer to 5+5 is 10.\\n- grammatical: The submission \\\"8\\\" is grammatically correct.\\n- logical: The submission \\\"8\\\" is not logical in the context of the given task, as 5+5 does not equal 8.\",\n",
      "  \"value\": \"N\",\n",
      "  \"score\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "custom_criteria = {\n",
    "    \"numeric\": \"Does the output contain numeric information?\",\n",
    "    \"mathematical\": \"Does the output contain mathematical information?\",\n",
    "    \"grammatical\": \"Is the output grammatically correct?\",\n",
    "    \"logical\": \"Is the output logical?\",\n",
    "}\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, llm=llm, criteria=custom_criteria)\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "  prediction=\"8\",\n",
    "  input=\"5+5?\",\n",
    ")\n",
    "print(json.dumps(eval_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b476dd6-917e-4dca-a14d-991046a57b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning\": \"1. Correctness:\\n- The submission states that the Moon is not the satellite of the earth.\\n- The reference clearly states that the Moon is the satellite of the earth.\\n- Therefore, the submission is incorrect based on the factual information provided in the reference.\",\n",
      "  \"value\": \"N\",\n",
      "  \"score\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(EvaluatorType.LABELED_CRITERIA, llm=llm, criteria=Criteria.CORRECTNESS)\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction = \"Moon is not the satellite of the earth\",\n",
    "    reference= \"Moon is the satellite of the earth\",\n",
    "    input = \"Name the satellite of the earth?\"\n",
    ")\n",
    "print(json.dumps(eval_result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
